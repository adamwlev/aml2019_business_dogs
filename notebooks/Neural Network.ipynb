{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def lower(word):\n",
    "    return word.lower()\n",
    "\n",
    "lemm = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    return lemm.lemmatize(word)\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "def no_punctuation(word):\n",
    "    return ''.join(c for c in word if c not in punct)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "def no_stop_words(word):\n",
    "    return word if word not in stop_words else ''\n",
    "\n",
    "strategy_map = {'lo':lower,'lem':lemmatize,\n",
    "                'punct':no_punctuation,'stop':no_stop_words}\n",
    "\n",
    "def preprocess(docs,strategies):\n",
    "    for strategy in strategies:\n",
    "        new_docs = []\n",
    "        for doc in docs:\n",
    "            new_doc = []\n",
    "            for word in doc:\n",
    "                transformed = strategy_map[strategy](word)\n",
    "                if transformed:\n",
    "                    new_doc.append(transformed)\n",
    "            new_docs.append(new_doc)\n",
    "        docs = new_docs\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train = []\n",
    "for i in range(10000):\n",
    "    with open('../descriptions_train/%d.txt' % (i,)) as f:\n",
    "        documents_train.append(f.read().split())\n",
    "documents_train = preprocess(documents_train,['lo','punct'])\n",
    "\n",
    "documents_test = []\n",
    "for i in range(2000):\n",
    "    with open('../descriptions_test/%d.txt' % (i,)) as f:\n",
    "        documents_test.append(f.read().split())\n",
    "documents_test = preprocess(documents_test,['lo','punct'])\n",
    "\n",
    "tag_docs_train = []\n",
    "for i in range(10000):\n",
    "    with open('../tags_train/%d.txt' % (i,),'r') as f:\n",
    "        tag_docs_train.append([word for line in f.read().split('\\n') for word in line.split(':') if word])\n",
    "tag_docs_train = preprocess(tag_docs_train,['lo','punct'])\n",
    "\n",
    "tag_docs_test = []\n",
    "for i in range(2000):\n",
    "    with open('../tags_test/%d.txt' % (i,),'r') as f:\n",
    "        tag_docs_test.append([word for line in f.read().split('\\n') for word in line.split(':') if word])\n",
    "tag_docs_test = preprocess(tag_docs_test,['lo','punct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stop_words,min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.fit([' '.join(doc) for copus in [documents_train, tag_docs_train, \n",
    "                                       documents_test, tag_docs_test] for doc in copus]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = np.array(tfidf.transform([' '.join(doc) for doc in documents_train]).todense())\n",
    "text_test = np.array(tfidf.transform([' '.join(doc) for doc in documents_test]).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_train = np.array(tfidf.transform([' '.join(doc) for doc in tag_docs_train]).todense())\n",
    "tags_test = np.array(tfidf.transform([' '.join(doc) for doc in tag_docs_test]).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5980), (2000, 5980))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train.shape,text_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5980), (2000, 5980))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_train.shape,tags_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5980.000000\n",
       "mean        0.000345\n",
       "std         0.004552\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         0.233815\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.abs(tags_test.mean(0))).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_train = pd.read_csv('../features_train/features_resnet1000intermediate_train.csv',header=None)\n",
    "feats_train_b = pd.read_csv('../features_train/features_resnet1000_train.csv',header=None)\n",
    "feats_test = pd.read_csv('../features_test/features_resnet1000intermediate_test.csv',header=None)\n",
    "feats_test_b = pd.read_csv('../features_test/features_resnet1000_test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images_train/5373.jpg</td>\n",
       "      <td>-0.899450</td>\n",
       "      <td>-0.930470</td>\n",
       "      <td>-2.503365</td>\n",
       "      <td>-3.172499</td>\n",
       "      <td>-2.819133</td>\n",
       "      <td>0.992159</td>\n",
       "      <td>-3.698863</td>\n",
       "      <td>0.619991</td>\n",
       "      <td>0.956148</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.021916</td>\n",
       "      <td>2.214253</td>\n",
       "      <td>-1.382491</td>\n",
       "      <td>1.672911</td>\n",
       "      <td>1.014233</td>\n",
       "      <td>2.599949</td>\n",
       "      <td>2.773284</td>\n",
       "      <td>-2.066632</td>\n",
       "      <td>0.385754</td>\n",
       "      <td>-3.241320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images_train/984.jpg</td>\n",
       "      <td>-1.346954</td>\n",
       "      <td>-3.119461</td>\n",
       "      <td>-0.765971</td>\n",
       "      <td>-1.382550</td>\n",
       "      <td>-1.104675</td>\n",
       "      <td>-3.656271</td>\n",
       "      <td>-4.815436</td>\n",
       "      <td>-0.556942</td>\n",
       "      <td>-1.402286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>-3.968805</td>\n",
       "      <td>-2.694711</td>\n",
       "      <td>-4.196480</td>\n",
       "      <td>-2.880234</td>\n",
       "      <td>-1.210742</td>\n",
       "      <td>-1.605143</td>\n",
       "      <td>-4.859987</td>\n",
       "      <td>-0.837670</td>\n",
       "      <td>-0.967604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images_train/7127.jpg</td>\n",
       "      <td>-3.445498</td>\n",
       "      <td>-1.524573</td>\n",
       "      <td>-1.001654</td>\n",
       "      <td>-3.668335</td>\n",
       "      <td>-1.805517</td>\n",
       "      <td>-1.633496</td>\n",
       "      <td>-7.127826</td>\n",
       "      <td>-1.147802</td>\n",
       "      <td>-1.055816</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.991777</td>\n",
       "      <td>-2.628053</td>\n",
       "      <td>-2.971074</td>\n",
       "      <td>-2.537039</td>\n",
       "      <td>-1.707429</td>\n",
       "      <td>1.013672</td>\n",
       "      <td>0.608460</td>\n",
       "      <td>-3.714998</td>\n",
       "      <td>-0.484735</td>\n",
       "      <td>0.138767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5     \\\n",
       "0  images_train/5373.jpg -0.899450 -0.930470 -2.503365 -3.172499 -2.819133   \n",
       "1   images_train/984.jpg -1.346954 -3.119461 -0.765971 -1.382550 -1.104675   \n",
       "2  images_train/7127.jpg -3.445498 -1.524573 -1.001654 -3.668335 -1.805517   \n",
       "\n",
       "       6         7         8         9     ...      991       992       993   \\\n",
       "0  0.992159 -3.698863  0.619991  0.956148  ... -3.021916  2.214253 -1.382491   \n",
       "1 -3.656271 -4.815436 -0.556942 -1.402286  ...  0.011003 -3.968805 -2.694711   \n",
       "2 -1.633496 -7.127826 -1.147802 -1.055816  ... -2.991777 -2.628053 -2.971074   \n",
       "\n",
       "       994       995       996       997       998       999       1000  \n",
       "0  1.672911  1.014233  2.599949  2.773284 -2.066632  0.385754 -3.241320  \n",
       "1 -4.196480 -2.880234 -1.210742 -1.605143 -4.859987 -0.837670 -0.967604  \n",
       "2 -2.537039 -1.707429  1.013672  0.608460 -3.714998 -0.484735  0.138767  \n",
       "\n",
       "[3 rows x 1001 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_train_b.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 2049), (2000, 2049))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_train.shape,feats_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pics_train = np.zeros((10000,3048))\n",
    "for _,row in feats_train.iterrows():\n",
    "    try:\n",
    "        i = int(row[0].split('/')[1].split('.jpg')[0])\n",
    "    except:\n",
    "        i = int(row[0].split('/')[1].split('..jpg')[0])\n",
    "    pics_train[i,:2048] = row.values[1:]\n",
    "for _,row in feats_train_b.iterrows():\n",
    "    try:\n",
    "        i = int(row[0].split('/')[1].split('.jpg')[0])\n",
    "    except:\n",
    "        i = int(row[0].split('/')[1].split('..jpg')[0])\n",
    "    pics_train[i,2048:] = row.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3048.000000\n",
       "mean        0.286300\n",
       "std         0.726504\n",
       "min        -3.890733\n",
       "25%         0.259784\n",
       "50%         0.384236\n",
       "75%         0.526699\n",
       "max         4.441978\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pics_train.mean(0)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pics_test = np.zeros((2000,3048))\n",
    "for _,row in feats_test.iterrows():\n",
    "    try:\n",
    "        i = int(row[0].split('/')[1].split('.jpg')[0])\n",
    "    except:\n",
    "        i = int(row[0].split('/')[1].split('..jpg')[0])\n",
    "    pics_test[i,:2048] = row.values[1:]\n",
    "for _,row in feats_test_b.iterrows():\n",
    "    try:\n",
    "        i = int(row[0].split('/')[1].split('.jpg')[0])\n",
    "    except:\n",
    "        i = int(row[0].split('/')[1].split('..jpg')[0])\n",
    "    pics_test[i,2048:] = row.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3048.000000\n",
       "mean        0.286300\n",
       "std         0.726504\n",
       "min        -3.890733\n",
       "25%         0.259784\n",
       "50%         0.384236\n",
       "75%         0.526699\n",
       "max         4.441978\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pics_train.mean(0)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pics_train = np.hstack([pics_train,tags_train])\n",
    "pics_test = np.hstack([pics_test,tags_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('text_train_full_wbert',text_train)\n",
    "np.save('pics_train_full_wbert',pics_train)\n",
    "np.save('text_test_full_wbert',text_test)\n",
    "np.save('pics_test_full_wbert',pics_test)\n",
    "# text_train = np.load('text_train_full.npy')\n",
    "# pics_train = np.load('pics_train_full.npy')\n",
    "# text_test = np.load('text_test_full.npy')\n",
    "# pics_test = np.load('pics_test_full.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vectors_desc_train = np.load('bert_desc_train.npy')\n",
    "bert_vectors_tags_train = np.load('bert_tags_train.npy')\n",
    "bert_vectors_desc_test = np.load('bert_desc_test.npy')\n",
    "bert_vectors_tags_test = np.load('bert_tags_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = np.hstack([text_train,bert_vectors_desc_train])\n",
    "text_test = np.hstack([text_test,bert_vectors_desc_test])\n",
    "pics_train = np.hstack([pics_train,bert_vectors_tags_train])\n",
    "pics_test = np.hstack([pics_test,bert_vectors_tags_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "pca.fit(pics_train)\n",
    "pics_train = pca.transform(pics_train)\n",
    "pics_test = pca.transform(pics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5510), (10000, 100))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train.shape,pics_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.tensor(X, requires_grad=True)\n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X,y,X_val,y_val,early_stop_window):\n",
    "    losses, train_ave, train_map, val_ave, val_map, n_epochs = {}, {}, {}, {}, {}, 0\n",
    "    \n",
    "    ds = PrepareData(X=X, y=y)\n",
    "    dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    D_in, H_1, D_out = X.shape[1], 2048, y.shape[1]\n",
    "    \n",
    "    model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H_1),\n",
    "#           torch.nn.Dropout(0.2),\n",
    "#           torch.nn.BatchNorm1d(H_1),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H_1, D_out),\n",
    "        ).to(device)\n",
    "    \n",
    "    def loss_fn(y_pred, y):\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        return 1 - cos(y_pred, y).mean()\n",
    "    #     return torch.norm((y_pred-y), p=2, dim=1).mean()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001, weight_decay=.2)\n",
    "    for t in range(100):\n",
    "        for ix, (_x, _y) in enumerate(dl):\n",
    "            _x = Variable(_x).float()\n",
    "            _y = Variable(_y).float()\n",
    "\n",
    "            y_pred = model(_x)\n",
    "\n",
    "            loss = loss_fn(y_pred, _y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        y_pred = model(Variable(ds.X).float())\n",
    "        train_loss = loss_fn(y_pred, Variable(ds.y).float()).data.numpy()\n",
    "        losses[t] = train_loss\n",
    "        print(t,train_loss)\n",
    "\n",
    "        ave, map_ = evaluate(y_pred.data.numpy(),ds.y.data.numpy())\n",
    "        train_ave[t] = ave * 2000/X.shape[0]\n",
    "        train_map[t] = map_ * X.shape[0]/2000\n",
    "        y_pred_val = model(Variable(torch.from_numpy(X_val)).float())\n",
    "        ave_val, map_val = evaluate(y_pred_val.data.numpy(),y_val)\n",
    "        val_ave[t] = ave_val * 2000/X_val.shape[0]\n",
    "        val_map[t] = map_val * X_val.shape[0]/2000\n",
    "        n_epochs = t\n",
    "        print(\"\"\"Iter %d: Train Ave Rank: %g, Train MAP@20: %g,\n",
    "        Val Ave Rank: %g, Val MAP@20: %g\"\"\" % (t,train_ave[t],train_map[t],val_ave[t],val_map[t]))\n",
    "        \n",
    "        if t>=early_stop_window:\n",
    "            if val_map[t]<val_map[t-early_stop_window]:\n",
    "                print(\"EARLY STOP TRIGGERED BECAUSE NO IMPROVEMENT FOR %d ITERATIONS\" % (early_stop_window,))\n",
    "                return model, losses, train_ave, train_map, val_ave, val_map, n_epochs-early_stop_window\n",
    "\n",
    "#         if (t+1)%15==0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 1.05\n",
    "    \n",
    "    return model, losses, train_ave, train_map, val_ave, val_map, n_epochs\n",
    "\n",
    "def predict(model,X):\n",
    "    return model(Variable(torch.from_numpy(X)).float()).data.numpy()\n",
    "\n",
    "def get_prediction(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics,metric='cosine')\n",
    "    return dists.argsort(1)\n",
    "\n",
    "def map_20(ranks):\n",
    "    return np.mean([(20-rank)/20 if rank<20 else 0 for rank in ranks])\n",
    "\n",
    "def evaluate(vectors,label_vectors):\n",
    "    preds = get_prediction(vectors,label_vectors)\n",
    "    ranks = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds)]\n",
    "    return np.mean(ranks),map_20(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.30330652\n",
      "Iter 0: Train Ave Rank: 51.4673, Train MAP@20: 0.83945,\n",
      "        Val Ave Rank: 65.483, Val MAP@20: 0.3203\n",
      "1 0.29690957\n",
      "Iter 1: Train Ave Rank: 48.6177, Train MAP@20: 0.822325,\n",
      "        Val Ave Rank: 63.887, Val MAP@20: 0.324125\n",
      "2 0.28876805\n",
      "Iter 2: Train Ave Rank: 44.3552, Train MAP@20: 0.871475,\n",
      "        Val Ave Rank: 60.292, Val MAP@20: 0.341375\n",
      "3 0.28191233\n",
      "Iter 3: Train Ave Rank: 45.6456, Train MAP@20: 0.869425,\n",
      "        Val Ave Rank: 63.6815, Val MAP@20: 0.338325\n",
      "4 0.26997995\n",
      "Iter 4: Train Ave Rank: 40.9449, Train MAP@20: 0.901575,\n",
      "        Val Ave Rank: 59.429, Val MAP@20: 0.33415\n",
      "5 0.26830435\n",
      "Iter 5: Train Ave Rank: 37.767, Train MAP@20: 0.9653,\n",
      "        Val Ave Rank: 55.6195, Val MAP@20: 0.35925\n",
      "6 0.26053423\n",
      "Iter 6: Train Ave Rank: 34.3361, Train MAP@20: 1.06693,\n",
      "        Val Ave Rank: 55.5045, Val MAP@20: 0.360075\n",
      "7 0.26160973\n",
      "Iter 7: Train Ave Rank: 33.3827, Train MAP@20: 1.10075,\n",
      "        Val Ave Rank: 52.7645, Val MAP@20: 0.366925\n",
      "8 0.24565911\n",
      "Iter 8: Train Ave Rank: 27.8765, Train MAP@20: 1.20412,\n",
      "        Val Ave Rank: 49.16, Val MAP@20: 0.3767\n",
      "9 0.24299502\n",
      "Iter 9: Train Ave Rank: 26.9635, Train MAP@20: 1.2751,\n",
      "        Val Ave Rank: 50.174, Val MAP@20: 0.383525\n",
      "10 0.23654598\n",
      "Iter 10: Train Ave Rank: 24.5703, Train MAP@20: 1.32523,\n",
      "        Val Ave Rank: 49.0375, Val MAP@20: 0.39905\n",
      "11 0.2341072\n",
      "Iter 11: Train Ave Rank: 22.595, Train MAP@20: 1.39162,\n",
      "        Val Ave Rank: 47.457, Val MAP@20: 0.396075\n",
      "12 0.22794265\n",
      "Iter 12: Train Ave Rank: 21.3815, Train MAP@20: 1.4871,\n",
      "        Val Ave Rank: 47.6565, Val MAP@20: 0.396675\n",
      "13 0.22449523\n",
      "Iter 13: Train Ave Rank: 19.0094, Train MAP@20: 1.57903,\n",
      "        Val Ave Rank: 43.8805, Val MAP@20: 0.41065\n",
      "14 0.21650928\n",
      "Iter 14: Train Ave Rank: 16.8099, Train MAP@20: 1.68087,\n",
      "        Val Ave Rank: 43.094, Val MAP@20: 0.408925\n",
      "15 0.20895189\n",
      "Iter 15: Train Ave Rank: 15.6673, Train MAP@20: 1.76567,\n",
      "        Val Ave Rank: 42.6365, Val MAP@20: 0.419775\n",
      "16 0.20824528\n",
      "Iter 16: Train Ave Rank: 15.4438, Train MAP@20: 1.81957,\n",
      "        Val Ave Rank: 43.161, Val MAP@20: 0.415075\n",
      "17 0.19871557\n",
      "Iter 17: Train Ave Rank: 12.8407, Train MAP@20: 1.99018,\n",
      "        Val Ave Rank: 42.5615, Val MAP@20: 0.430775\n",
      "18 0.19394487\n",
      "Iter 18: Train Ave Rank: 11.8972, Train MAP@20: 2.05258,\n",
      "        Val Ave Rank: 40.7095, Val MAP@20: 0.436525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a90ccd419ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpics_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     rets = fit(text_train[train_index],pics_train[train_index],\n\u001b[0;32m----> 6\u001b[0;31m                text_train[test_index],pics_train[test_index],5)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mn_epochs_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ec2502d82b5e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(X, y, X_val, y_val, early_stop_window)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amlftw/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5,shuffle=True)\n",
    "n_epochs_results = []\n",
    "val_map_results = []\n",
    "for train_index,test_index in cv.split(text_train,pics_train):\n",
    "    rets = fit(text_train[train_index],pics_train[train_index],\n",
    "               text_train[test_index],pics_train[test_index],5)\n",
    "    model, losses, train_ave, train_map, val_ave, val_map, n_epochs = rets\n",
    "    n_epochs_results.append(n_epochs)\n",
    "    val_map_results.append(val_map)\n",
    "    vectors = predict(model,text_train[test_index])\n",
    "    print('************Fold eval*************')\n",
    "    print(evaluate(vectors,pics_train[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_results,np.mean(n_epochs_results),val_map_results,np.mean(val_map_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_train = np.load('text_train_full.npy')\n",
    "pics_train = np.load('pics_train_full.npy')\n",
    "text_test = np.load('text_test_full.npy')\n",
    "pics_test = np.load('pics_test_full.npy')\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(pics_train[:,2048:])\n",
    "pics_train = pca.transform(pics_train[:,2048:])\n",
    "pics_test = pca.transform(pics_test[:,2048:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5510), (10000, 100))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train.shape,pics_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.29487443\n",
      "Iter 0: Train Ave Rank: 55.3416, Train MAP@20: 0.81165,\n",
      "        Val Ave Rank: 68.8935, Val MAP@20: 0.315675\n",
      "1 0.29591763\n",
      "Iter 1: Train Ave Rank: 54.9973, Train MAP@20: 0.7379,\n",
      "        Val Ave Rank: 71.6695, Val MAP@20: 0.29415\n",
      "2 0.28796297\n",
      "Iter 2: Train Ave Rank: 49.1473, Train MAP@20: 0.793925,\n",
      "        Val Ave Rank: 65.9995, Val MAP@20: 0.311475\n",
      "3 0.2841556\n",
      "Iter 3: Train Ave Rank: 46.5553, Train MAP@20: 0.806475,\n",
      "        Val Ave Rank: 65.3375, Val MAP@20: 0.3127\n",
      "4 0.2745517\n",
      "Iter 4: Train Ave Rank: 46.2772, Train MAP@20: 0.865425,\n",
      "        Val Ave Rank: 63.9725, Val MAP@20: 0.33015\n",
      "5 0.26491535\n",
      "Iter 5: Train Ave Rank: 37.3458, Train MAP@20: 1.00233,\n",
      "        Val Ave Rank: 57.8535, Val MAP@20: 0.3528\n",
      "6 0.2597258\n",
      "Iter 6: Train Ave Rank: 38.3665, Train MAP@20: 0.973325,\n",
      "        Val Ave Rank: 59.308, Val MAP@20: 0.34655\n",
      "7 0.25363666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a90ccd419ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpics_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     rets = fit(text_train[train_index],pics_train[train_index],\n\u001b[0;32m----> 6\u001b[0;31m                text_train[test_index],pics_train[test_index],5)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mn_epochs_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ec2502d82b5e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(X, y, X_val, y_val, early_stop_window)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mtrain_ave\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mave\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtrain_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ec2502d82b5e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(vectors, label_vectors)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ec2502d82b5e>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(vecs, pics)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cosine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmap_20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5,shuffle=True)\n",
    "n_epochs_results = []\n",
    "val_map_results = []\n",
    "for train_index,test_index in cv.split(text_train,pics_train):\n",
    "    rets = fit(text_train[train_index],pics_train[train_index],\n",
    "               text_train[test_index],pics_train[test_index],5)\n",
    "    model, losses, train_ave, train_map, val_ave, val_map, n_epochs = rets\n",
    "    n_epochs_results.append(n_epochs)\n",
    "    val_map_results.append(val_map)\n",
    "    vectors = predict(model,text_train[test_index])\n",
    "    print('************Fold eval*************')\n",
    "    print(evaluate(vectors,pics_train[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_results,np.mean(n_epochs_results),val_map_results,np.mean(val_map_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fit(text_train,pics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics,metric='cosine')\n",
    "    return dists.argsort(1)\n",
    "\n",
    "def get_top_20(descr_id):\n",
    "    return preds[descr_id][:20]\n",
    "\n",
    "def save_submission():\n",
    "    data = []\n",
    "    for i in range(2000):\n",
    "        data.append(['%d.txt' % (i,),' '.join('%d.jpg' % (pic_id,) for pic_id in get_top_20(i))])\n",
    "    pd.DataFrame(data,columns=['Descritpion_ID','Top_20_Image_IDs']).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = predict(model,text_test)\n",
    "preds = get_prediction(vecs,pics_test)\n",
    "save_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "import nltk\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for train data\n",
    "desc_files = len(os.listdir('../descriptions_train'))\n",
    "all_desc_train = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_train/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_train.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for train data\n",
    "tag_files = len(os.listdir('../tags_train'))\n",
    "all_tags_train = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_train/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_train.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for test data\n",
    "desc_files = len(os.listdir('../descriptions_test'))\n",
    "all_desc_test = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_test/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_test.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for test data\n",
    "tag_files = len(os.listdir('../tags_test'))\n",
    "all_tags_test = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_test/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_test.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "all_docs.extend(all_desc_train)\n",
    "all_docs.extend(all_desc_test)\n",
    "all_docs.extend(all_tags_train)\n",
    "all_docs.extend(all_tags_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "vectorizer.fit(all_docs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = vectorizer.transform(all_desc_train)\n",
    "test_desc = vectorizer.transform(all_desc_test)\n",
    "train_tags = vectorizer.transform(all_tags_train)\n",
    "test_tags = vectorizer.transform(all_tags_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.array(train_desc.todense())\n",
    "test_desc = np.array(test_desc.todense())\n",
    "train_tags = np.array(train_tags.todense())\n",
    "test_tags = np.array(test_tags.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = pd.read_csv('../features_train/features_resnet1000_train.csv', header=None)\n",
    "train_2048 = pd.read_csv('../features_train/features_resnet1000intermediate_train.csv', header=None)\n",
    "test_1000 = pd.read_csv('../features_test/features_resnet1000_test.csv', header=None)\n",
    "test_2048 = pd.read_csv('../features_test/features_resnet1000intermediate_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(string):\n",
    "    string = string.replace('.', ' ').replace('/', ' ')\n",
    "    num = [int(s) for s in string.split() if s.isdigit()]\n",
    "    return num[0]\n",
    "\n",
    "def parse_to_numpy(pd):\n",
    "    images_idx = []\n",
    "    for string in pd[0]:\n",
    "        images_idx.append(get_num(string))\n",
    "\n",
    "    pd.insert(1, \"Image_Index\", images_idx, True)\n",
    "    pd = pd.sort_values(by=['Image_Index'])\n",
    "    pd = pd.reset_index(drop=True)\n",
    "    del pd['Image_Index']\n",
    "    del pd[0]\n",
    "    np = pd.to_numpy()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = parse_to_numpy(train_1000)\n",
    "train_2048 = parse_to_numpy(train_2048)\n",
    "test_1000 = parse_to_numpy(test_1000)\n",
    "test_2048 = parse_to_numpy(test_2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pic = np.hstack((train_1000, train_2048))#, train_tags))\n",
    "test_pic = np.hstack((test_1000, test_2048))#, test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google embedding\n",
    "#tf.enable_eager_execution()\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "### Overwrite old train_desc and test_desc\n",
    "train_desc_tf = embed(all_desc_train).numpy()\n",
    "test_desc_tf = embed(all_desc_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "# path = get_tmpfile(\"word2vec.model\")\n",
    "# model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "# model.train([all_desc_train], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5931), (10000, 3048), (2000, 5931), (2000, 3048))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape, test_desc.shape, test_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set PCA Dimensions\n",
    "pca = PCA(n_components = 100)\n",
    "pca.fit(train_pic)\n",
    "train_pic = pca.transform(train_pic)\n",
    "test_pic = pca.transform(test_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_desc.shape, train_pic.shape, test_desc.shape, test_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vecs,pics):\n",
    "    assert vecs[0].shape[0]==pics.shape[0]\n",
    "    dists = np.zeros((pics.shape[0],pics.shape[0]))\n",
    "    for vec in vecs:\n",
    "        dists += pairwise_distances(vec,pics,metric='cosine')\n",
    "    dists = dists/len(vecs)\n",
    "    return dists.argsort(1)\n",
    "def map_20(ranks):\n",
    "    return np.mean([(20-rank)/20 if rank<20 else 0 for rank in ranks])\n",
    "def evaluate(vectors,label_vectors):\n",
    "    preds = get_prediction(vectors,label_vectors)\n",
    "    ranks = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds)]\n",
    "    return np.mean(ranks),map_20(ranks)\n",
    "def get_top_20(descr_id):\n",
    "    return preds[descr_id][:20]\n",
    "def save_submission():\n",
    "    data = []\n",
    "    for i in range(2000):\n",
    "        data.append(['%d.txt' % (i,),' '.join('%d.jpg' % (pic_id,) for pic_id in get_top_20(i))])\n",
    "    pd.DataFrame(data,columns=['Descritpion_ID','Top_20_Image_IDs']).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34.9365, 0.506475)\n",
      "(36.5815, 0.5003749999999999)\n",
      "(35.4395, 0.495025)\n",
      "(33.8835, 0.48664999999999997)\n",
      "(36.488, 0.4963)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "regr = Ridge()\n",
    "\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    regr.fit(train_desc[train_index], train_pic[train_index])\n",
    "    pred = regr.predict(train_desc[test_index])\n",
    "    output = evaluate([pred], train_pic[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36.097, 0.48264999999999997)\n",
      "(36.131, 0.48297500000000004)\n",
      "(35.9415, 0.480125)\n",
      "(34.4185, 0.4703)\n",
      "(37.097, 0.47382500000000005)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "regr = Ridge()\n",
    "\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    regr.fit(train_desc[train_index], train_pic[train_index])\n",
    "    pred = regr.predict(train_desc[test_index])\n",
    "    regr.fit(train_desc_tf[train_index], train_pic[train_index])\n",
    "    pred2 = regr.predict(train_desc_tf[test_index])\n",
    "    output = evaluate([pred,pred2], train_pic[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33.367, 0.51175)\n",
      "(33.469, 0.51195)\n",
      "(33.8685, 0.5110750000000001)\n",
      "(31.6135, 0.49965)\n",
      "(34.629, 0.506875)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "regr = Ridge()\n",
    "train_desc = np.hstack([train_desc,train_desc_tf])\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    regr.fit(train_desc[train_index], train_pic[train_index])\n",
    "    pred = regr.predict(train_desc[test_index])\n",
    "    output = evaluate([pred], train_pic[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(interaction_only=True,include_bias=False)\n",
    "train_desc_b = poly.fit_transform(train_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regr.fit(train_desc, train_pic)\n",
    "# prediction = regr.predict(test_desc)\n",
    "# preds = get_prediction(prediction, test_pic)\n",
    "# save_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "('tfidf',TfidfVectorizer(stop_words=stop_words, min_df=2)),\n",
    "('ridge',Ridge())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37.6075, 0.48457500000000003)\n",
      "(36.19, 0.50375)\n",
      "(33.9805, 0.48605)\n",
      "(35.2325, 0.509925)\n",
      "(33.248, 0.49900000000000005)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "all_desc_train = np.array(all_desc_train)\n",
    "all_desc_test = np.array(all_desc_test)\n",
    "\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    pipeline.fit(all_desc_train[train_index], train_pic[train_index])\n",
    "    pred = pipeline.predict(all_desc_train[test_index])\n",
    "    output = evaluate(pred, train_pic[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

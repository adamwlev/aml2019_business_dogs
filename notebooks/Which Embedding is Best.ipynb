{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def lower(word):\n",
    "    return word.lower()\n",
    "\n",
    "lemm = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    return lemm.lemmatize(word)\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "def no_punctuation(word):\n",
    "    return ''.join(c for c in word if c not in punct)\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "def no_stop_words(word):\n",
    "    return word if word not in stop_words else ''\n",
    "\n",
    "strategy_map = {'lo':lower,'lem':lemmatize,\n",
    "                'punct':no_punctuation,'stop':no_stop_words}\n",
    "\n",
    "def preprocess(docs,strategies):\n",
    "    for strategy in strategies:\n",
    "        new_docs = []\n",
    "        for doc in docs:\n",
    "            new_doc = []\n",
    "            for word in doc:\n",
    "                transformed = strategy_map[strategy](word)\n",
    "                if transformed:\n",
    "                    new_doc.append(transformed)\n",
    "            new_docs.append(new_doc)\n",
    "        docs = new_docs\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train = [] ## list of word\n",
    "for i in range(10000):\n",
    "    with open('../descriptions_train/%d.txt' % (i,)) as f:\n",
    "        documents_train.append(f.read().split())\n",
    "documents_train = preprocess(documents_train,['lo','punct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_docs_train = [] ## list of word\n",
    "for i in range(10000):\n",
    "    with open('../tags_train/%d.txt' % (i,),'r') as f:\n",
    "        tag_docs_train.append([word for line in f.read().split('\\n') for word in line.split(':') if word])\n",
    "\n",
    "tag_docs_train = preprocess(tag_docs_train,['lo','punct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics,metric='cosine')\n",
    "    return dists.argsort(1)\n",
    "\n",
    "def evaluate(preds):\n",
    "    ranks = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds)]\n",
    "    map_20 = np.mean([(20-rank)/20 if rank<20 else 0 for rank in ranks])\n",
    "    ave_rank = np.mean(ranks)\n",
    "    return map_20, ave_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stop_words,min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit([' '.join(doc) for doc in documents_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_desc = tfidf.transform([' '.join(doc) for doc in documents_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5508)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tags = tfidf.transform([' '.join(doc) for doc in tag_docs_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5508)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_prediction(vectors_desc,vectors_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09830499999999999, 824.3905)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_docs = []\n",
    "for i in range(10000):\n",
    "    gensim_docs.append(TaggedDocument([word for word in documents_train[i] if word not in stop_words],[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(gensim_docs, vector_size=300, window=8, \n",
    "                min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(gensim_docs,total_examples=model.corpus_count,epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_desc = np.array([model[i] for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tags = np.zeros((10000,300))\n",
    "for i in range(10000):\n",
    "    vectors_tags[i] = model.infer_vector([word for word in tag_docs_train[i] if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_prediction(vectors_desc,vectors_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.026189999999999998, 2642.3745)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert https://readthedocs.org/projects/bert-as-service/downloads/pdf/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient('54.159.74.226')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train = [] ## reprocess for bert\n",
    "for i in range(10000):\n",
    "    with open('../descriptions_train/%d.txt' % (i,)) as f:\n",
    "        documents_train.append(' '.join(s.replace('.','').strip().lower() for s in f.read().split('\\n')))\n",
    "        \n",
    "tag_docs_train = [] ## reprocess for bert\n",
    "for i in range(10000):\n",
    "    with open('../tags_train/%d.txt' % (i,),'r') as f:\n",
    "        tag_docs_train.append(' '.join([word.lower().strip()\n",
    "                               for line in f.read().split('\\n')\n",
    "                               for word in line.split(':') if word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_desc = bc.encode(documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tags = set(i for i in range(10000) if not tag_docs_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = bc.encode([tag_docs_train[i] for i in range(10000) if i not in no_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tags = np.zeros((10000,1024))\n",
    "counter = 0\n",
    "for i in range(10000):\n",
    "    if i not in no_tags:\n",
    "        vectors_tags[i] = res[counter]\n",
    "        counter += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_prediction(vectors_desc,vectors_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.017285, 3296.9714)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test = [] ## reprocess for bert\n",
    "for i in range(2000):\n",
    "    with open('../descriptions_test/%d.txt' % (i,)) as f:\n",
    "        documents_test.append(' '.join(s.replace('.','').strip().lower() for s in f.read().split('\\n')))\n",
    "        \n",
    "tag_docs_test = [] ## reprocess for bert\n",
    "for i in range(2000):\n",
    "    with open('../tags_test/%d.txt' % (i,),'r') as f:\n",
    "        tag_docs_test.append(' '.join([word.lower().strip()\n",
    "                               for line in f.read().split('\\n')\n",
    "                               for word in line.split(':') if word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_desc_test = bc.encode(documents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tags_test = set(i for i in range(2000) if not tag_docs_test[i])\n",
    "res = bc.encode([tag_docs_test[i] for i in range(2000) if i not in no_tags_test])\n",
    "vectors_tags_test = np.zeros((2000,1024))\n",
    "counter = 0\n",
    "for i in range(2000):\n",
    "    if i not in no_tags_test:\n",
    "        vectors_tags_test[i] = res[counter]\n",
    "        counter += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bert_desc_train',vectors_desc)\n",
    "np.save('bert_tags_train',vectors_tags)\n",
    "np.save('bert_desc_test',vectors_desc_test)\n",
    "np.save('bert_tags_test',vectors_tags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3fd1696151a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msplitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplitted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "with open('glove.6B/glove.6B.300d.txt','r') as f:\n",
    "    for line in f.read().split('\\n'):\n",
    "        if not line: continue\n",
    "        splitted = line.split()\n",
    "        word = splitted[0]\n",
    "        vector = np.array([float(n) for n in splitted[1:]])\n",
    "        assert len(vector)==300\n",
    "        word_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_word_dict = set()\n",
    "all_words = set(word for corpus in [documents_train,tag_docs_train] for doc in corpus for word in doc)\n",
    "for word in all_words:\n",
    "    if word in word_dict:\n",
    "        in_word_dict.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9031"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9876"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_desc = np.zeros((10000,300))\n",
    "for i in range(10000):\n",
    "    words = [word for word in documents_train[i] if word not in stop_words]\n",
    "    some_word = False\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            some_word = True\n",
    "    if not some_word:\n",
    "        continue\n",
    "    doc_matrix = np.array([word_dict[word] for word in words if word in word_dict])\n",
    "    vectors_desc[i] = doc_matrix.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_tags = np.zeros((10000,300))\n",
    "for i in range(10000):\n",
    "    words = [word for word in tag_docs_train[i] if word not in stop_words]\n",
    "    some_word = False\n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            some_word = True\n",
    "    if not some_word:\n",
    "        continue\n",
    "    doc_matrix = np.array([word_dict[word] for word in words if word in word_dict])\n",
    "    vectors_tags[i] = doc_matrix.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = get_prediction(vectors_desc,vectors_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.086125, 928.6632)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import itertools\n",
    "from copy import deepcopy as copy\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for train data\n",
    "desc_files = len(os.listdir('../descriptions_train'))\n",
    "all_desc_train = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_train/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_train.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for train data\n",
    "tag_files = len(os.listdir('../tags_train'))\n",
    "all_tags_train = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_train/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_train.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for test data\n",
    "desc_files = len(os.listdir('../descriptions_test'))\n",
    "all_desc_test = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_test/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_test.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for test data\n",
    "tag_files = len(os.listdir('../tags_test'))\n",
    "all_tags_test = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_test/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_test.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "all_docs.extend(all_desc_train)\n",
    "all_docs.extend(all_desc_test)\n",
    "all_docs.extend(all_tags_train)\n",
    "all_docs.extend(all_tags_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = pd.read_csv('../features_train/features_resnet1000_train.csv', header=None)\n",
    "train_2048 = pd.read_csv('../features_train/features_resnet1000intermediate_train.csv', header=None)\n",
    "test_1000 = pd.read_csv('../features_test/features_resnet1000_test.csv', header=None)\n",
    "test_2048 = pd.read_csv('../features_test/features_resnet1000intermediate_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(string):\n",
    "    string = string.replace('.', ' ').replace('/', ' ')\n",
    "    num = [int(s) for s in string.split() if s.isdigit()]\n",
    "    return num[0]\n",
    "\n",
    "def parse_to_numpy(pd):\n",
    "    images_idx = []\n",
    "    for string in pd[0]:\n",
    "        images_idx.append(get_num(string))\n",
    "\n",
    "    pd.insert(1, \"Image_Index\", images_idx, True)\n",
    "    pd = pd.sort_values(by=['Image_Index'])\n",
    "    pd = pd.reset_index(drop=True)\n",
    "    del pd['Image_Index']\n",
    "    del pd[0]\n",
    "    np = pd.to_numpy()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = parse_to_numpy(train_1000)\n",
    "train_2048 = parse_to_numpy(train_2048)\n",
    "test_1000 = parse_to_numpy(test_1000)\n",
    "test_2048 = parse_to_numpy(test_2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google embedding\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "train_desc_tf = embed(all_desc_train).numpy()\n",
    "test_desc_tf = embed(all_desc_test).numpy()\n",
    "train_tags_tf = embed(all_tags_train).numpy()\n",
    "test_tags_tf = embed(all_tags_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = all_desc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The skateboarder is putting on a show using the picnic table as his stage. A skateboarder pulling tricks on top of a picnic table. A man riding on a skateboard on top of a table. A skate boarder doing a trick on a picnic table. A person is riding a skateboard on a picnic table with a crowd watching. '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The skateboarder is putting on a show using the picnic table as his stage A skateboarder pulling tricks on top of a picnic table A man riding on a skateboard on top of a table A skate boarder doing a trick on a picnic table A person is riding a skateboard on a picnic table with a crowd watching '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punctuation(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skateboarder putting show using picnic table stage. skateboarder pulling tricks top picnic table. man riding skateboard top table. skate boarder trick picnic table. person riding skateboard picnic table crowd watching.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stops(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skateboarder show picnic table stage skateboarder tricks top picnic table man skateboard top table skate boarder trick picnic table person skateboard picnic table crowd watching'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_non_nouns(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_non_nouns('we woman went')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "def no_punctuation(word):\n",
    "    return ''.join(c for c in word if c not in punct)\n",
    "\n",
    "def remove_stops(doc):\n",
    "    return ' '.join(word for word in doc.split() if no_punctuation(word).lower() not in stop_words)\n",
    "\n",
    "def stem_doc(doc):\n",
    "    return ' '.join(stemmer.stem(word) for word in doc.split())\n",
    "\n",
    "def filter_non_nouns(doc):\n",
    "    blob = TextBlob(doc)\n",
    "    nouns = [noun[0] for noun in filter(lambda x:x[1] in ['NN', \"NNS\"], blob.tags)]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "def get_tfidf_vectors(stem,filter_nn,sublin_tf,ngram_tfidf,pca_comps):\n",
    "    desc_train_ = copy(all_desc_train)\n",
    "    desc_test_ = copy(all_desc_test)\n",
    "    tags_train_ = copy(all_tags_train)\n",
    "    tags_test_ = copy(all_tags_test)\n",
    "    \n",
    "    desc_train_ = [remove_stops(doc) for doc in desc_train_]\n",
    "    desc_test_ = [remove_stops(doc) for doc in desc_test_]\n",
    "    tags_train_ = [remove_stops(doc) for doc in tags_train_]\n",
    "    tags_test_ = [remove_stops(doc) for doc in tags_test_]\n",
    "    \n",
    "    if stem:\n",
    "        desc_train_ = [stem_doc(doc) for doc in desc_train_]\n",
    "        desc_test_ = [stem_doc(doc) for doc in desc_test_]\n",
    "        tags_train_ = [stem_doc(doc) for doc in tags_train_]\n",
    "        tags_test_ = [stem_doc(doc) for doc in tags_test_]\n",
    "    \n",
    "    if filter_nn:\n",
    "        desc_train_ = [filter_non_nouns(doc) for doc in desc_train_]\n",
    "        desc_test_ = [filter_non_nouns(doc) for doc in desc_test_]\n",
    "        tags_train_ = [filter_non_nouns(doc) for doc in tags_train_]\n",
    "        tags_test_ = [filter_non_nouns(doc) for doc in tags_test_]\n",
    "    \n",
    "    train_docs_ = desc_train_ + tags_train_\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2,sublinear_tf=sublin_tf,\n",
    "                                 ngram_range=(1,2) if ngram_tfidf else (1,1))\n",
    "    vectorizer.fit(train_docs_)\n",
    "\n",
    "    train_desc_bow = np.array(vectorizer.transform(desc_train_).todense())\n",
    "    test_desc_bow = np.array(vectorizer.transform(desc_test_).todense())\n",
    "    train_tags_bow = np.array(vectorizer.transform(tags_train_).todense())\n",
    "    test_tags_bow = np.array(vectorizer.transform(tags_test_).todense())\n",
    "    \n",
    "    pca = PCA(n_components=pca_comps)\n",
    "    pca.fit(np.vstack([train_desc_bow,test_desc_bow,train_tags_bow,test_tags_bow]))\n",
    "    train_desc_bow_pca = pca.transform(train_desc_bow)\n",
    "    test_desc_bow_pca = pca.transform(test_desc_bow)\n",
    "    train_tags_bow_pca = pca.transform(train_tags_bow)\n",
    "    test_tags_bow_pca = pca.transform(test_tags_bow)\n",
    "    \n",
    "    return train_desc_bow_pca, test_desc_bow_pca, train_tags_bow_pca, test_tags_bow_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "word2vec_embed = hub.load(\"https://tfhub.dev/google/Wiki-words-500/2\")\n",
    "\n",
    "def get_word2vec_vectors(filter_nn,sublin_tf):\n",
    "    desc_train_ = copy(all_desc_train)\n",
    "    desc_test_ = copy(all_desc_test)\n",
    "    tags_train_ = copy(all_tags_train)\n",
    "    tags_test_ = copy(all_tags_test)\n",
    "    \n",
    "    desc_train_ = [no_punctuation(doc).lower() for doc in desc_train_]\n",
    "    desc_test_ = [no_punctuation(doc).lower() for doc in desc_test_]\n",
    "    tags_train_ = [no_punctuation(doc).lower() for doc in tags_train_]\n",
    "    tags_test_ = [no_punctuation(doc).lower() for doc in tags_test_]\n",
    "    \n",
    "    desc_train_ = [remove_stops(doc) for doc in desc_train_]\n",
    "    desc_test_ = [remove_stops(doc) for doc in desc_test_]\n",
    "    tags_train_ = [remove_stops(doc) for doc in tags_train_]\n",
    "    tags_test_ = [remove_stops(doc) for doc in tags_test_]\n",
    "    \n",
    "    if filter_nn:\n",
    "        desc_train_ = [filter_non_nouns(doc) for doc in desc_train_]\n",
    "        desc_test_ = [filter_non_nouns(doc) for doc in desc_test_]\n",
    "        tags_train_ = [filter_non_nouns(doc) for doc in tags_train_]\n",
    "        tags_test_ = [filter_non_nouns(doc) for doc in tags_test_]\n",
    "    \n",
    "    word2vec_docs = desc_train_ + desc_test_ + tags_train_ + tags_test_\n",
    "    w2v_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=1)\n",
    "    w2v_vectorizer.fit(word2vec_docs)\n",
    "    \n",
    "    word2vec_train_desc = np.zeros((10000,500))\n",
    "    for i in range(10000):\n",
    "        doc = desc_train_[i]\n",
    "        words = doc.split()\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            if sublin_tf:\n",
    "                weight = (1+np.log(word_counter[word]))*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            else:\n",
    "                weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_train_desc[i] += weight*np.ravel(word2vec_embed([word]))\n",
    "            total += weight\n",
    "        word2vec_train_desc[i] /= total\n",
    "\n",
    "    word2vec_test_desc = np.zeros((2000,500))\n",
    "    for i in range(2000):\n",
    "        doc = desc_test_[i]\n",
    "        words = doc.split()\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            if sublin_tf:\n",
    "                weight = (1+np.log(word_counter[word]))*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            else:\n",
    "                weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_test_desc[i] += weight*np.ravel(word2vec_embed([word]))\n",
    "            total += weight\n",
    "        word2vec_test_desc[i] /= total\n",
    "    \n",
    "    word2vec_train_tags = np.zeros((10000,500))\n",
    "    for i in range(10000):\n",
    "        doc = tags_train_[i]\n",
    "        words = doc.split()\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            if sublin_tf:\n",
    "                weight = (1+np.log(word_counter[word]))*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            else:\n",
    "                weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_train_tags[i] += weight*np.ravel(word2vec_embed([word]))\n",
    "            total += weight\n",
    "        if total!=0:\n",
    "            word2vec_train_tags[i] /= total\n",
    "\n",
    "    word2vec_test_tags = np.zeros((2000,500))\n",
    "    for i in range(2000):\n",
    "        doc = tags_test_[i]\n",
    "        words = doc.split()\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            if sublin_tf:\n",
    "                weight = (1+np.log(word_counter[word]))*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            else:\n",
    "                weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_test_tags[i] += weight*np.ravel(word2vec_embed([word]))\n",
    "            total += weight\n",
    "        if total!=0:\n",
    "            word2vec_test_tags[i] /= total\n",
    "    \n",
    "    return word2vec_train_desc, word2vec_test_desc, word2vec_train_tags, word2vec_test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_cos(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics,metric='cosine')\n",
    "    return dists.T.argsort(1)\n",
    "def get_prediction_euc(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics)\n",
    "    return dists.T.argsort(1)\n",
    "def map_20(ranks):\n",
    "    return np.mean([(20-rank)/20 if rank<20 else 0 for rank in ranks])\n",
    "def map_20_2(ranks):\n",
    "    return np.mean([1/(1+rank) if rank<20 else 0 for rank in ranks])\n",
    "def evaluate(vectors,label_vectors,w1,w2,w3):\n",
    "    vectors = np.copy(vectors)\n",
    "    label_vectors = np.copy(label_vectors)\n",
    "    vectors[:,:512] = w1*vectors[:,:512]\n",
    "    vectors[:,512:1212] = w2*vectors[:,512:1212]\n",
    "    vectors[:,1212:] = w3*vectors[:,1212:]\n",
    "    vectors = vectors/(w1+w2+w3)\n",
    "    label_vectors[:,:512] = w1*label_vectors[:,:512]\n",
    "    label_vectors[:,512:1212] = w2*label_vectors[:,512:1212]\n",
    "    label_vectors[:,1212:] = w3*label_vectors[:,1212:]\n",
    "    label_vectors = label_vectors/(w1+w2+w3)\n",
    "    preds1 = get_prediction_cos(vectors,label_vectors)\n",
    "    ranks1 = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds1)]\n",
    "    preds2 = get_prediction_euc(vectors,label_vectors)\n",
    "    ranks2 = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds2)]\n",
    "    return np.mean(ranks1),np.mean(ranks2),map_20(ranks1),map_20(ranks2),map_20_2(ranks1),map_20_2(ranks2)\n",
    "def get_top_20(descr_id):\n",
    "    return preds[descr_id][:20]\n",
    "def save_submission():\n",
    "    data = []\n",
    "    for i in range(2000):\n",
    "        data.append(['%d.txt' % (i,),' '.join('%d.jpg' % (pic_id,) for pic_id in get_top_20(i))])\n",
    "    pd.DataFrame(data,columns=['Descritpion_ID','Top_20_Image_IDs']).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.2965, 4.2435, 0.8232, 0.8284, 0.5730184995517038, 0.5768876451936437)\n",
      "(4.0505, 3.7445, 0.8275249999999998, 0.836825, 0.5833570839527689, 0.5892947439235172)\n",
      "(4.4555, 4.343, 0.8230999999999999, 0.82685, 0.5830236257765282, 0.5891563196542879)\n",
      "(3.8695, 3.7345, 0.831225, 0.836475, 0.579873676654443, 0.5809441326904948)\n",
      "(4.2235, 4.112, 0.8278, 0.8299249999999999, 0.5828499868230324, 0.5819055849744511)\n",
      "[((True, True, False, 900), 0.5883695643739644), ((True, True, False, 700), 0.5880004074464408), ((True, True, False, 1100), 0.5874003503892049), ((True, True, True, 1100), 0.5843608096143621), ((True, True, True, 900), 0.5836769117333475), ((False, True, True, 700), 0.5836376852872789), ((True, True, True, 700), 0.5814386591646228), ((True, False, True, 700), 0.5693471030337052)]\n",
      "(4.293, 4.2345, 0.82325, 0.82845, 0.57414600576582, 0.5782496102903804)\n",
      "(4.0195, 3.726, 0.82905, 0.837475, 0.5852342213879171, 0.5898927006730799)\n",
      "(4.424, 4.3015, 0.823875, 0.827975, 0.5842359075522001, 0.5899019066266551)\n",
      "(3.844, 3.705, 0.8323999999999999, 0.83795, 0.5838871001397125, 0.583970774330159)\n",
      "(4.183, 4.066, 0.829175, 0.831625, 0.586449717405058, 0.5848953306454467)\n",
      "[((True, True, False, 900), 0.5883695643739644), ((True, True, False, 700), 0.5880004074464408), ((True, True, False, 1100), 0.5874003503892049), ((False, True, True, 900), 0.5853820645131442), ((True, True, True, 1100), 0.5843608096143621), ((True, True, True, 900), 0.5836769117333475), ((False, True, True, 700), 0.5836376852872789), ((True, True, True, 700), 0.5814386591646228), ((True, False, True, 700), 0.5693471030337052)]\n",
      "(4.285, 4.245, 0.824025, 0.8289000000000001, 0.5751452777743411, 0.5800899337032076)\n",
      "(4.0075, 3.729, 0.829275, 0.8371, 0.584527143764818, 0.5904945968462222)\n",
      "(4.4345, 4.296, 0.8238, 0.8284, 0.5825986346213126, 0.5898291366163075)\n",
      "(3.8415, 3.6905, 0.8327749999999998, 0.8383999999999999, 0.5848478337172803, 0.5874325440147228)\n",
      "(4.175, 4.056, 0.8297, 0.8321999999999999, 0.5880849569097054, 0.5866314665790865)\n",
      "[((True, True, False, 900), 0.5883695643739644), ((True, True, False, 700), 0.5880004074464408), ((True, True, False, 1100), 0.5874003503892049), ((False, True, True, 1100), 0.5868955355519094), ((False, True, True, 900), 0.5853820645131442), ((True, True, True, 1100), 0.5843608096143621), ((True, True, True, 900), 0.5836769117333475), ((False, True, True, 700), 0.5836376852872789), ((True, True, True, 700), 0.5814386591646228), ((True, False, True, 700), 0.5693471030337052)]\n",
      "(4.219, 4.1825, 0.82585, 0.8316, 0.57841217896096, 0.5841929413573183)\n",
      "(3.9835, 3.6735, 0.830525, 0.839275, 0.5860931188114431, 0.5921078982850654)\n",
      "(4.407, 4.287, 0.82455, 0.829525, 0.589534908813237, 0.5947651295342085)\n",
      "(3.8095, 3.648, 0.83465, 0.8417749999999999, 0.5878363930552591, 0.5905248571324615)\n",
      "(4.158, 4.034, 0.829725, 0.8323499999999999, 0.5894921774175257, 0.5900599181412842)\n",
      "[((False, True, False, 700), 0.5903301488900675), ((True, True, False, 900), 0.5883695643739644), ((True, True, False, 700), 0.5880004074464408), ((True, True, False, 1100), 0.5874003503892049), ((False, True, True, 1100), 0.5868955355519094), ((False, True, True, 900), 0.5853820645131442), ((True, True, True, 1100), 0.5843608096143621), ((True, True, True, 900), 0.5836769117333475), ((False, True, True, 700), 0.5836376852872789), ((True, True, True, 700), 0.5814386591646228), ((True, False, True, 700), 0.5693471030337052)]\n",
      "(4.2405, 4.2115, 0.82575, 0.8309249999999999, 0.5789813553104961, 0.5838506410406759)\n",
      "(3.9885, 3.6855, 0.8302249999999999, 0.83875, 0.586467211407014, 0.5912773518728434)\n",
      "(4.4275, 4.3095, 0.8235999999999999, 0.8285999999999999, 0.5884642880232082, 0.5933986133190855)\n",
      "(3.822, 3.655, 0.8343499999999999, 0.84115, 0.586983409693162, 0.5897028308851452)\n",
      "(4.151, 4.0115, 0.82955, 0.833025, 0.5893628315075876, 0.5913556484429743)\n",
      "[((False, True, False, 700), 0.5903301488900675), ((False, True, False, 900), 0.5899170171121447), ((True, True, False, 900), 0.5883695643739644), ((True, True, False, 700), 0.5880004074464408), ((True, True, False, 1100), 0.5874003503892049), ((False, True, True, 1100), 0.5868955355519094), ((False, True, True, 900), 0.5853820645131442), ((True, True, True, 1100), 0.5843608096143621), ((True, True, True, 900), 0.5836769117333475), ((False, True, True, 700), 0.5836376852872789), ((True, True, True, 700), 0.5814386591646228), ((True, False, True, 700), 0.5693471030337052)]\n",
      "(4.2775, 4.2255, 0.825, 0.8311499999999999, 0.5791579183308092, 0.5842772698062172)\n",
      "(4.005, 3.683, 0.8300250000000001, 0.8388749999999999, 0.5871011894087165, 0.5916285819937716)\n",
      "(4.451, 4.336, 0.8229, 0.8281499999999998, 0.5890138109783234, 0.5933268392576635)\n",
      "(3.82, 3.6405, 0.8345999999999999, 0.8417, 0.5881507396155616, 0.5913682291714992)\n",
      "(4.1625, 4.0215, 0.829275, 0.83285, 0.5897107483052723, 0.5901078510069222)\n",
      "[((False, True, False, 700), 0.5903301488900675), ((False, True, False, 1100), 0.5901417542472147), ((False, True, False, 900), 0.5899170171121447), ((True, True, False, 900), 0.5883695643739644), ((True, True, False, 700), 0.5880004074464408), ((True, True, False, 1100), 0.5874003503892049), ((False, True, True, 1100), 0.5868955355519094), ((False, True, True, 900), 0.5853820645131442), ((True, True, True, 1100), 0.5843608096143621), ((True, True, True, 900), 0.5836769117333475), ((False, True, True, 700), 0.5836376852872789), ((True, True, True, 700), 0.5814386591646228), ((True, False, True, 700), 0.5693471030337052)]\n"
     ]
    }
   ],
   "source": [
    "stem_tfidf = [True,False]\n",
    "filter_nn_tfidf = [False]\n",
    "sublin_tf_tfidf = [True]\n",
    "ngram_tfidf = [True,False]\n",
    "filter_nn_w2v = [False]\n",
    "sublin_tf_w2v = [True]\n",
    "pca_comps = [700,900,1100]\n",
    "#results = {}\n",
    "#tfidf_cache, w2v_cache = {}, {}\n",
    "for st_ti,fn_ti,sl_ti,ng_ti,n_c,fn_wv,sl_wv in itertools.product(stem_tfidf,filter_nn_tfidf,\n",
    "                                                                 sublin_tf_tfidf,ngram_tfidf,pca_comps,\n",
    "                                                                 filter_nn_w2v,sublin_tf_w2v):\n",
    "    gc.collect()\n",
    "    \n",
    "    if (st_ti,sl_ti,ng_ti,n_c) in results:\n",
    "        continue\n",
    "    \n",
    "    if (st_ti,fn_ti,sl_ti,ng_ti,n_c) in tfidf_cache:\n",
    "        train_desc_bow_pca, train_tags_bow_pca = tfidf_cache[(st_ti,fn_ti,sl_ti,ng_ti,n_c)]\n",
    "    else:\n",
    "        train_desc_bow_pca, _, train_tags_bow_pca, _ = get_tfidf_vectors(st_ti,fn_ti,sl_ti,ng_ti,n_c)\n",
    "        tfidf_cache[(st_ti,fn_ti,sl_ti,ng_ti,n_c)] = (train_desc_bow_pca, train_tags_bow_pca)\n",
    "    \n",
    "    if (fn_wv,sl_wv) in w2v_cache:\n",
    "        word2vec_train_desc, word2vec_train_tags = w2v_cache[(fn_wv,sl_wv)]\n",
    "    else:\n",
    "        word2vec_train_desc, _, word2vec_train_tags, _ = get_word2vec_vectors(fn_wv,sl_wv)\n",
    "        w2v_cache[(fn_wv,sl_wv)] = (word2vec_train_desc, word2vec_train_tags)\n",
    "    \n",
    "    train_desc = np.hstack((train_desc_tf, train_desc_bow_pca, word2vec_train_desc))\n",
    "    train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca, word2vec_train_tags))\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "    res_ = []\n",
    "    for train_index, test_index in kf.split(train_pic):\n",
    "        rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "        pred = rcv.predict(train_pic[test_index])\n",
    "        output = evaluate(pred, train_desc[test_index])\n",
    "        print(output)\n",
    "        res_.append(output[-1])\n",
    "    \n",
    "    results[(st_ti,sl_ti,ng_ti,n_c)] = np.mean(res_)\n",
    "    print(sorted(results.items(),key=lambda x: x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.4195, 4.396, 0.821425, 0.8272, 0.5748273957702944, 0.5778663969243689)\n",
      "(4.099, 3.7885, 0.8257000000000001, 0.835875, 0.5815175083903025, 0.5841478713258706)\n",
      "(4.5585, 4.44, 0.818425, 0.8235750000000001, 0.5836825363383608, 0.5863966236034347)\n",
      "(3.9135, 3.7205, 0.8317749999999999, 0.8389, 0.5822963375397392, 0.5840442069948456)\n",
      "(4.3575, 4.1735, 0.82325, 0.8274, 0.579281077558922, 0.579444107584452)\n"
     ]
    }
   ],
   "source": [
    "train_desc_bow_pca, train_tags_bow_pca = tfidf_cache[(False,False,True,False,700)]\n",
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca))\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca))\n",
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "res_ = []\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.301, 4.249, 0.82425, 0.8305, 0.5784054270505896, 0.5842499941707758)\n",
      "(4.0135, 3.698, 0.830025, 0.838275, 0.5866917884338743, 0.590748007410546)\n",
      "(4.458, 4.341, 0.822625, 0.828, 0.5893651392553095, 0.5947456455760441)\n",
      "(3.8415, 3.6515, 0.834125, 0.8412000000000001, 0.5888657914045877, 0.5911152591947096)\n",
      "(4.17, 4.0355, 0.828625, 0.83175, 0.5882588127837934, 0.5894257836590654)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5900569380022282"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc_bow_pca, _, train_tags_bow_pca, _ = get_tfidf_vectors(False,False,True,False,1300)\n",
    "word2vec_train_desc, word2vec_train_tags = w2v_cache[(False,True)]\n",
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca, word2vec_train_desc))\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca, word2vec_train_tags))\n",
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "res_ = []\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)\n",
    "    res_.append(output[-1])\n",
    "np.mean(res_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print\n",
      "print\n",
      "print\n",
      "print\n",
      "print\n"
     ]
    }
   ],
   "source": [
    "train_desc_bow_pca, train_tags_bow_pca = tfidf_cache[(False,False,True,False,700)]\n",
    "word2vec_train_desc, word2vec_train_tags = w2v_cache[(False,True)]\n",
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca, word2vec_train_desc))\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca, word2vec_train_tags))\n",
    "\n",
    "weights_ = [1,2,3]\n",
    "results_weightings = defaultdict(list)\n",
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    print('print')\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    for w1,w2,w3 in itertools.product(*[weights_]*3):\n",
    "        output = evaluate(pred, train_desc[test_index],w1,w2,w3)\n",
    "        results_weightings[(w1,w2,w3)].append(output[-1])\n",
    "\n",
    "for w1,w2,w3 in itertools.product(*[weights_]*3):\n",
    "    results_weightings[(w1,w2,w3)] = np.mean(results_weightings[(w1,w2,w3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2, 2, 3), 0.5919601457211519),\n",
       " ((2, 3, 3), 0.5911941395249057),\n",
       " ((1, 1, 1), 0.5903301488900675),\n",
       " ((2, 2, 2), 0.5903301488900675),\n",
       " ((3, 3, 3), 0.5903301488900675),\n",
       " ((1, 1, 2), 0.5896408128438468),\n",
       " ((2, 3, 2), 0.587138431424097),\n",
       " ((3, 3, 2), 0.586284240767832),\n",
       " ((1, 2, 2), 0.5860891286628748),\n",
       " ((1, 2, 3), 0.5848957465492024),\n",
       " ((2, 2, 1), 0.584596070082738),\n",
       " ((2, 3, 1), 0.5832316105544783),\n",
       " ((3, 3, 1), 0.5829515530582249),\n",
       " ((3, 2, 3), 0.5829358850531994),\n",
       " ((1, 2, 1), 0.5787123629170108),\n",
       " ((3, 2, 2), 0.5781396779433157),\n",
       " ((2, 1, 3), 0.5778691071883912),\n",
       " ((1, 3, 3), 0.5770755709740036),\n",
       " ((2, 1, 2), 0.5745975562724169),\n",
       " ((3, 2, 1), 0.5738383507703168),\n",
       " ((1, 3, 2), 0.5715706832645339),\n",
       " ((3, 1, 3), 0.5675047832800155),\n",
       " ((2, 1, 1), 0.5672605879560756),\n",
       " ((1, 1, 3), 0.566640583877337),\n",
       " ((1, 3, 1), 0.5649502234117791),\n",
       " ((3, 1, 2), 0.5621398572355576),\n",
       " ((3, 1, 1), 0.5560987050213289)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(results_weightings.items(),key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc_bow_pca, test_desc_bow_pca, train_tags_bow_pca, test_tags_bow_pca = get_tfidf_vectors(False,False,True,False,700)\n",
    "word2vec_train_desc, word2vec_test_desc, word2vec_train_tags, word2vec_test_tags = get_word2vec_vectors(False,True)\n",
    "\n",
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow_pca, word2vec_test_desc))\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow_pca, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1712), (2000, 1712), (10000, 2712), (2000, 2712))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, test_desc.shape, train_pic.shape, test_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_wrapper(vectors, label_vectors, w1, w2, w3):\n",
    "    vectors = np.copy(vectors)\n",
    "    label_vectors = np.copy(label_vectors)\n",
    "    vectors[:,:512] = w1*vectors[:,:512]\n",
    "    vectors[:,512:1212] = w2*vectors[:,512:1212]\n",
    "    vectors[:,1212:] = w3*vectors[:,1212:]\n",
    "    vectors = vectors/(w1+w2+w3)\n",
    "    label_vectors[:,:512] = w1*label_vectors[:,:512]\n",
    "    label_vectors[:,512:1212] = w2*label_vectors[:,512:1212]\n",
    "    label_vectors[:,1212:] = w3*label_vectors[:,1212:]\n",
    "    label_vectors = label_vectors/(w1+w2+w3)\n",
    "    return vectors,label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "reg.fit(train_pic, train_desc)\n",
    "print(\"best reg:\",reg.alpha_)\n",
    "prediction = reg.predict(test_pic)\n",
    "prediction_, test_desc_ = weight_wrapper(prediction, test_desc, 2, 2, 3)\n",
    "preds = get_prediction_euc(prediction_, test_desc_)\n",
    "save_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try:\n",
    "* Filtering out non-nouns before TFIDF\n",
    "* Filtering out non-nounds for word2vec\n",
    "* Stemming before TFIDF\n",
    "* Sublinear TF in tfidf\n",
    "* Sublinear TF in word2vec\n",
    "* Number of PCA dims for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

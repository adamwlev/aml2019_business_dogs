{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for train data\n",
    "desc_files = len(os.listdir('../descriptions_train'))\n",
    "all_desc_train = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_train/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_train.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for train data\n",
    "tag_files = len(os.listdir('../tags_train'))\n",
    "all_tags_train = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_train/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_train.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for test data\n",
    "desc_files = len(os.listdir('../descriptions_test'))\n",
    "all_desc_test = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_test/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_test.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for test data\n",
    "tag_files = len(os.listdir('../tags_test'))\n",
    "all_tags_test = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_test/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_test.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "all_docs.extend(all_desc_train)\n",
    "all_docs.extend(all_desc_test)\n",
    "all_docs.extend(all_tags_train)\n",
    "all_docs.extend(all_tags_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = pd.read_csv('../features_train/features_resnet1000_train.csv', header=None)\n",
    "train_2048 = pd.read_csv('../features_train/features_resnet1000intermediate_train.csv', header=None)\n",
    "test_1000 = pd.read_csv('../features_test/features_resnet1000_test.csv', header=None)\n",
    "test_2048 = pd.read_csv('../features_test/features_resnet1000intermediate_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(string):\n",
    "    string = string.replace('.', ' ').replace('/', ' ')\n",
    "    num = [int(s) for s in string.split() if s.isdigit()]\n",
    "    return num[0]\n",
    "\n",
    "def parse_to_numpy(pd):\n",
    "    images_idx = []\n",
    "    for string in pd[0]:\n",
    "        images_idx.append(get_num(string))\n",
    "\n",
    "    pd.insert(1, \"Image_Index\", images_idx, True)\n",
    "    pd = pd.sort_values(by=['Image_Index'])\n",
    "    pd = pd.reset_index(drop=True)\n",
    "    del pd['Image_Index']\n",
    "    del pd[0]\n",
    "    np = pd.to_numpy()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = parse_to_numpy(train_1000)\n",
    "train_2048 = parse_to_numpy(train_2048)\n",
    "test_1000 = parse_to_numpy(test_1000)\n",
    "test_2048 = parse_to_numpy(test_2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google embedding\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "train_desc_tf = embed(all_desc_train).numpy()\n",
    "test_desc_tf = embed(all_desc_test).numpy()\n",
    "train_tags_tf = embed(all_tags_train).numpy()\n",
    "test_tags_tf = embed(all_tags_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "vectorizer.fit(all_desc_train+all_tags_train)\n",
    "\n",
    "train_desc_bow = np.array(vectorizer.transform(all_desc_train).todense())\n",
    "test_desc_bow = np.array(vectorizer.transform(all_desc_test).todense())\n",
    "train_tags_bow = np.array(vectorizer.transform(all_tags_train).todense())\n",
    "test_tags_bow = np.array(vectorizer.transform(all_tags_test).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('businesses', 0.6623775362968445),\n",
       " ('busines', 0.6080313324928284),\n",
       " ('busi_ness', 0.5612965226173401),\n",
       " ('PETER_PASSI_covers', 0.5530025959014893),\n",
       " ('Business', 0.5466139316558838),\n",
       " ('businesss', 0.5441080331802368),\n",
       " ('Sopris_supplemental_solutions', 0.5252544283866882),\n",
       " ('company', 0.5192004442214966),\n",
       " ('entrepreneurial', 0.5077816247940063),\n",
       " ('buiness', 0.5039401650428772)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['business'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.8680489659309387),\n",
       " ('canines', 0.8181710839271545),\n",
       " ('cats', 0.76517653465271),\n",
       " ('pit_bulls', 0.7548302412033081),\n",
       " ('pets', 0.7424418330192566),\n",
       " ('puppies', 0.7385991811752319),\n",
       " ('pooches', 0.7162366509437561),\n",
       " ('German_shepherds', 0.7071062922477722),\n",
       " ('animals', 0.6985694169998169),\n",
       " ('pit_bull', 0.6983613967895508)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['dogs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "def process_doc(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = ''.join(c for c in doc if c not in punct)\n",
    "    doc = doc.split()\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    return doc\n",
    "\n",
    "gensim_docs = [process_doc(doc) for doc in all_docs]\n",
    "w2v_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=1)\n",
    "w2v_vectorizer.fit([' '.join(doc) for doc in gensim_docs]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_train_desc = np.zeros((10000,300))\n",
    "for i in range(10000):\n",
    "    with open(f'../descriptions_train/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in model:\n",
    "                continue\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_train_desc[i] += weight*model[word]\n",
    "            total += weight\n",
    "        word2vec_train_desc[i] /= total\n",
    "\n",
    "word2vec_test_desc = np.zeros((2000,300))\n",
    "for i in range(2000):\n",
    "    with open(f'../descriptions_test/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in model:\n",
    "                continue\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_test_desc[i] += weight*model[word]\n",
    "            total += weight\n",
    "        word2vec_test_desc[i] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_train_tags = np.zeros((10000,300))\n",
    "for i in range(10000):\n",
    "    with open(f'../tags_train/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in model:\n",
    "                continue\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_train_tags[i] += weight*model[word]\n",
    "            total += weight\n",
    "        if total!=0:\n",
    "            word2vec_train_tags[i] /= total\n",
    "\n",
    "word2vec_test_tags = np.zeros((2000,300))\n",
    "for i in range(2000):\n",
    "    with open(f'../tags_test/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in model:\n",
    "                continue\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_test_tags[i] += weight*model[word]\n",
    "            total += weight\n",
    "        if total!=0:\n",
    "            word2vec_test_tags[i] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics,metric='cosine')\n",
    "    return dists.T.argsort(1)\n",
    "def map_20(ranks):\n",
    "    return np.mean([(20-rank)/20 if rank<20 else 0 for rank in ranks])\n",
    "def map_20_2(ranks):\n",
    "    return np.mean([1/(1+rank) if rank<20 else 0 for rank in ranks])\n",
    "def evaluate(vectors,label_vectors):\n",
    "    preds = get_prediction(vectors,label_vectors)\n",
    "    ranks = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds)]\n",
    "    return np.mean(ranks),map_20(ranks),map_20_2(ranks)\n",
    "def get_top_20(descr_id):\n",
    "    return preds[descr_id][:20]\n",
    "def save_submission():\n",
    "    data = []\n",
    "    for i in range(2000):\n",
    "        data.append(['%d.txt' % (i,),' '.join('%d.jpg' % (pic_id,) for pic_id in get_top_20(i))])\n",
    "    pd.DataFrame(data,columns=['Descritpion_ID','Top_20_Image_IDs']).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Embeddings + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, train_desc_bow))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5978), (10000, 6978))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.958, 0.8067000000000001, 0.5458061875624376)\n",
      "(4.63, 0.8069, 0.5504180882348645)\n",
      "(5.12, 0.799225, 0.555200701010376)\n",
      "(4.3825, 0.8147249999999999, 0.551081020881853)\n",
      "(4.858, 0.803575, 0.554460956346715)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Embeddings + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, word2vec_test_desc))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 812), (10000, 1812))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.0055, 0.80035, 0.5467757744942536)\n",
      "(4.765, 0.8067000000000001, 0.5548446938961451)\n",
      "(4.9585, 0.80355, 0.5544075930669776)\n",
      "(4.4525, 0.814, 0.5525422264590415)\n",
      "(4.8345, 0.806575, 0.5593533226212212)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take TF Embeddings for Tags out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, word2vec_test_desc))\n",
    "\n",
    "train_pic = np.hstack((train_1000, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 812), (10000, 1300))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11.4715, 0.6714, 0.4094239396933476)\n",
      "(11.0405, 0.684125, 0.4217868812968078)\n",
      "(10.7035, 0.6812999999999999, 0.42777061354967705)\n",
      "(10.64, 0.68385, 0.4121403403807235)\n",
      "(11.044, 0.67745, 0.41653187822454457)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything Bagel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, train_desc_bow, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow, word2vec_test_desc))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 6278), (10000, 7278))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.7705, 0.808175, 0.5553373424541574)\n",
      "(4.483, 0.816, 0.5670722440463732)\n",
      "(4.764, 0.8096749999999999, 0.5668929112124544)\n",
      "(4.34, 0.81875, 0.5615508883230632)\n",
      "(4.6065, 0.8129, 0.5650779923658212)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 6278), (2000, 6278), (10000, 7278), (2000, 7278))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, test_desc.shape, train_pic.shape, test_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best reg: 7.157894736842106\n"
     ]
    }
   ],
   "source": [
    "rcv.fit(train_desc, train_pic)\n",
    "print('best reg:',rcv.alpha_)\n",
    "prediction = rcv.predict(test_desc)\n",
    "preds = get_prediction(prediction, test_pic)\n",
    "save_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

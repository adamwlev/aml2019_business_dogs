{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for train data\n",
    "desc_files = len(os.listdir('../descriptions_train'))\n",
    "all_desc_train = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_train/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_train.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for train data\n",
    "tag_files = len(os.listdir('../tags_train'))\n",
    "all_tags_train = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_train/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_train.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Description for test data\n",
    "desc_files = len(os.listdir('../descriptions_test'))\n",
    "all_desc_test = []\n",
    "\n",
    "for i in range(desc_files):\n",
    "    empty_str = ''\n",
    "    for line in open(f'../descriptions_test/{i}.txt'):\n",
    "        empty_str += line.replace('\\n',' ')\n",
    "    all_desc_test.append(empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tags for test data\n",
    "tag_files = len(os.listdir('../tags_test'))\n",
    "all_tags_test = []\n",
    "\n",
    "for i in range(tag_files):\n",
    "    nouns = ''\n",
    "    for line in open(f'../tags_test/{i}.txt'):\n",
    "        nouns += line.replace(':',' ')\n",
    "    all_tags_test.append(nouns.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "all_docs.extend(all_desc_train)\n",
    "all_docs.extend(all_desc_test)\n",
    "all_docs.extend(all_tags_train)\n",
    "all_docs.extend(all_tags_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = pd.read_csv('../features_train/features_resnet1000_train.csv', header=None)\n",
    "train_2048 = pd.read_csv('../features_train/features_resnet1000intermediate_train.csv', header=None)\n",
    "test_1000 = pd.read_csv('../features_test/features_resnet1000_test.csv', header=None)\n",
    "test_2048 = pd.read_csv('../features_test/features_resnet1000intermediate_test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num(string):\n",
    "    string = string.replace('.', ' ').replace('/', ' ')\n",
    "    num = [int(s) for s in string.split() if s.isdigit()]\n",
    "    return num[0]\n",
    "\n",
    "def parse_to_numpy(pd):\n",
    "    images_idx = []\n",
    "    for string in pd[0]:\n",
    "        images_idx.append(get_num(string))\n",
    "\n",
    "    pd.insert(1, \"Image_Index\", images_idx, True)\n",
    "    pd = pd.sort_values(by=['Image_Index'])\n",
    "    pd = pd.reset_index(drop=True)\n",
    "    del pd['Image_Index']\n",
    "    del pd[0]\n",
    "    np = pd.to_numpy()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1000 = parse_to_numpy(train_1000)\n",
    "train_2048 = parse_to_numpy(train_2048)\n",
    "test_1000 = parse_to_numpy(test_1000)\n",
    "test_2048 = parse_to_numpy(test_2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google embedding\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "train_desc_tf = embed(all_desc_train).numpy()\n",
    "test_desc_tf = embed(all_desc_test).numpy()\n",
    "train_tags_tf = embed(all_tags_train).numpy()\n",
    "test_tags_tf = embed(all_tags_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=2)\n",
    "vectorizer.fit(all_desc_train+all_tags_train)\n",
    "\n",
    "train_desc_bow = np.array(vectorizer.transform(all_desc_train).todense())\n",
    "test_desc_bow = np.array(vectorizer.transform(all_desc_test).todense())\n",
    "train_tags_bow = np.array(vectorizer.transform(all_tags_train).todense())\n",
    "test_tags_bow = np.array(vectorizer.transform(all_tags_test).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pca bow\n",
    "pca = PCA(n_components=700)\n",
    "pca.fit(np.vstack([train_desc_bow,test_desc_bow,train_tags_bow,test_tags_bow]))\n",
    "train_desc_bow_pca = pca.transform(train_desc_bow)\n",
    "test_desc_bow_pca = pca.transform(test_desc_bow)\n",
    "train_tags_bow_pca = pca.transform(train_tags_bow)\n",
    "test_tags_bow_pca = pca.transform(test_tags_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-500/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "def process_doc(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = ''.join(c for c in doc if c not in punct)\n",
    "    doc = doc.split()\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    return doc\n",
    "\n",
    "gensim_docs = [process_doc(doc) for doc in all_docs]\n",
    "w2v_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=1)\n",
    "w2v_vectorizer.fit([' '.join(doc) for doc in gensim_docs]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_train_desc = np.zeros((10000,500))\n",
    "for i in range(10000):\n",
    "    with open(f'../descriptions_train/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_train_desc[i] += weight*np.ravel(embed([word]))\n",
    "            total += weight\n",
    "        word2vec_train_desc[i] /= total\n",
    "\n",
    "word2vec_test_desc = np.zeros((2000,500))\n",
    "for i in range(2000):\n",
    "    with open(f'../descriptions_test/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_test_desc[i] += weight*np.ravel(embed([word]))\n",
    "            total += weight\n",
    "        word2vec_test_desc[i] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_train_tags = np.zeros((10000,500))\n",
    "for i in range(10000):\n",
    "    with open(f'../tags_train/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_train_tags[i] += weight*np.ravel(embed([word]))\n",
    "            total += weight\n",
    "        if total!=0:\n",
    "            word2vec_train_tags[i] /= total\n",
    "\n",
    "word2vec_test_tags = np.zeros((2000,500))\n",
    "for i in range(2000):\n",
    "    with open(f'../tags_test/{i}.txt') as f:\n",
    "        text = f.read()\n",
    "        words = process_doc(text)\n",
    "        num_words = len(words)\n",
    "        word_counter = Counter(words)\n",
    "        total = 0\n",
    "        for word in set(words):\n",
    "            if word not in w2v_vectorizer.vocabulary_:\n",
    "                continue\n",
    "            index = w2v_vectorizer.vocabulary_[word]\n",
    "            weight = word_counter[word]*w2v_vectorizer.idf_[index] ## tfidf weight\n",
    "            word2vec_test_tags[i] += weight*np.ravel(embed([word]))\n",
    "            total += weight\n",
    "        if total!=0:\n",
    "            word2vec_test_tags[i] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_cos(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics,metric='cosine')\n",
    "    return dists.T.argsort(1)\n",
    "def get_prediction_euc(vecs,pics):\n",
    "    dists = pairwise_distances(vecs,pics)\n",
    "    return dists.T.argsort(1)\n",
    "def map_20(ranks):\n",
    "    return np.mean([(20-rank)/20 if rank<20 else 0 for rank in ranks])\n",
    "def map_20_2(ranks):\n",
    "    return np.mean([1/(1+rank) if rank<20 else 0 for rank in ranks])\n",
    "def evaluate(vectors,label_vectors):\n",
    "    preds1 = get_prediction_cos(vectors,label_vectors)\n",
    "    ranks1 = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds1)]\n",
    "    preds2 = get_prediction_euc(vectors,label_vectors)\n",
    "    ranks2 = [np.argwhere(vec==i)[0][0] for i,vec in enumerate(preds2)]\n",
    "    return np.mean(ranks1),np.mean(ranks2),map_20(ranks1),map_20(ranks2),map_20_2(ranks1),map_20_2(ranks2)\n",
    "def get_top_20(descr_id):\n",
    "    return preds[descr_id][:20]\n",
    "def save_submission():\n",
    "    data = []\n",
    "    for i in range(2000):\n",
    "        data.append(['%d.txt' % (i,),' '.join('%d.jpg' % (pic_id,) for pic_id in get_top_20(i))])\n",
    "    pd.DataFrame(data,columns=['Descritpion_ID','Top_20_Image_IDs']).to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Embeddings + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, train_desc_bow))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5978), (10000, 6978))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.958, 5.003, 0.8067000000000001, 0.81235, 0.5458061875624376, 0.5554347617080202)\n",
      "(4.63, 4.3325, 0.8069, 0.81825, 0.5504180882348645, 0.556622007876884)\n",
      "(5.12, 5.0345, 0.799225, 0.8049, 0.555200701010376, 0.5608859265476526)\n",
      "(4.3825, 4.229, 0.8147249999999999, 0.8221, 0.551081020881853, 0.5552122167113073)\n",
      "(4.858, 4.764, 0.803575, 0.80675, 0.554460956346715, 0.5553660265624468)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Embeddings + TFIDF PCAd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow_pca))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1212), (10000, 2212))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.794, 4.85, 0.8108249999999999, 0.81535, 0.5513848795640204, 0.5568980977871456)\n",
      "(4.482, 4.2205, 0.8118249999999999, 0.821175, 0.5575023814206089, 0.5641014727038528)\n",
      "(5.005, 4.939, 0.8029749999999999, 0.807825, 0.5587217228999931, 0.5648855124579583)\n",
      "(4.285, 4.171, 0.8172999999999999, 0.8240250000000001, 0.5522328608912587, 0.5577769350102941)\n",
      "(4.7755, 4.7175, 0.8072, 0.809, 0.558855467859454, 0.5576404582990109)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Embeddings + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, word2vec_test_desc))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1012), (10000, 2012))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.1355, 5.0555, 0.79675, 0.7998999999999998, 0.5430441106859257, 0.5435238922197514)\n",
      "(4.7865, 4.5125, 0.8018, 0.8072, 0.5528397467320262, 0.5594502346037175)\n",
      "(5.1275, 4.9315, 0.8004, 0.803825, 0.5506896367972413, 0.5569001756542391)\n",
      "(4.464, 4.235, 0.8119, 0.8175249999999998, 0.5522892924842615, 0.5581437879887571)\n",
      "(4.948, 4.8145, 0.80415, 0.80745, 0.5489755291427698, 0.5480194601408224)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything Bagel with 700 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow_pca, word2vec_test_desc))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow_pca, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1712), (10000, 2712))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.671, 4.6505, 0.8122750000000001, 0.81655, 0.5555129951189163, 0.5615943896810963)\n",
      "(4.3545, 4.1025, 0.8169500000000001, 0.8251499999999999, 0.5648822333024732, 0.5688281261072089)\n",
      "(4.784, 4.7145, 0.8095249999999998, 0.814175, 0.5675986218159206, 0.5728940486435649)\n",
      "(4.1455, 4.0895, 0.822075, 0.827275, 0.5590299921226004, 0.5624668474155703)\n",
      "(4.598, 4.551, 0.8126499999999999, 0.814575, 0.5640659474877547, 0.564204467582641)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.36842105263158"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything Bagel with 500 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow_pca, word2vec_test_desc))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow_pca, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1512), (10000, 2512))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, train_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.6755, 4.654, 0.81235, 0.81615, 0.5548479792202122, 0.5618349306674577)\n",
      "(4.381, 4.157, 0.8161499999999999, 0.824275, 0.5614261666180396, 0.5675478446239863)\n",
      "(4.8125, 4.7415, 0.8095, 0.8138, 0.5660363064137445, 0.5724699297950071)\n",
      "(4.152, 4.1055, 0.82155, 0.826825, 0.5588742560372204, 0.562250288361879)\n",
      "(4.616, 4.5915, 0.8121499999999999, 0.813025, 0.5618326458113609, 0.5600093713905633)\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "rcv = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "for train_index, test_index in kf.split(train_pic):\n",
    "    rcv.fit(train_pic[train_index], train_desc[train_index])\n",
    "    pred = rcv.predict(train_pic[test_index])\n",
    "    output = evaluate(pred, train_desc[test_index])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Back to 700 for the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = np.hstack((train_desc_tf, train_desc_bow_pca, word2vec_train_desc))\n",
    "test_desc = np.hstack((test_desc_tf, test_desc_bow_pca, word2vec_test_desc))\n",
    "\n",
    "train_pic = np.hstack((train_1000, train_tags_tf, train_tags_bow_pca, word2vec_train_tags))\n",
    "test_pic = np.hstack((test_1000, test_tags_tf, test_tags_bow_pca, word2vec_test_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1712), (2000, 1712), (10000, 2712), (2000, 2712))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desc.shape, test_desc.shape, train_pic.shape, test_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best reg: 15.36842105263158\n"
     ]
    }
   ],
   "source": [
    "reg = RidgeCV(alphas=np.linspace(1,40,20))\n",
    "reg.fit(train_pic, train_desc)\n",
    "print(\"best reg:\",reg.alpha_)\n",
    "prediction = reg.predict(test_pic)\n",
    "preds = get_prediction_euc(prediction, test_desc)\n",
    "save_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
